**Introduction**

- This module can be used to export all of the Private Pages of a Liferay Site to HTML.
- The module exposes a custom Gogo shell command: siteCrawler:crawlPrivatePages which calls layoutLocalService.getLayouts(siteId, true) to get the list of Private Pages. For each Private Page it saves a HTML file based on the response from a standard authenticated HTTP Get request.

**Usage**

The syntax and arguments to call Gogo shell command are as follows:

```
siteCrawler:crawlPrivatePages "[companyId]" "[siteId]" "[layoutUrlPrefix]" "[emailAddress]" "[emailAddressEnc]" "[passwordEnc]" "[cookieDomain]" "[outputBaseFolder]"
```

For example:

```
siteCrawler:crawlPrivatePages "23990396268826" "32920" "https://webserver-lctmwklmsitescraper-prd.lfr.cloud/group/mw" "test@liferay.com" "677a746b7976694c6447763272666c7658754f5167413d3d" "6b6467536d6d766b48684e63772f427451596b4e62513d3d" "webserver-lctmwklmsitescraper-prd.lfr.cloud" "/mnt/persistent-storage/"
```

Note: 
- All arguments are passed as String values with quotes and a space separator between arguments.
- In a High Availability (i.e. clustered environment) the output may only be created on the node the Gogo shell is run on depending on outputBaseFolder. The Gogo shell command can be run from the Liferay service shell to control which node is used.

Arguments:

- companyId: The companyId of the Virtual Instance that the Site resides in.
- siteId: The siteId of the Site to be crawled.
- layoutUrlPrefix: The base URL used when accessing the Site e.g. http://mw.com:8080/group/intranet
- emailAddress: The email address of the user to log in as. See 'Crawler User Account' section.
- emailAddressEnc: The encrypted email address of the user. See 'Crawler User Account' section.
- passwordEnc: The encrypted password of the user. See 'Crawler User Account' section.
- cookieDomain: The Cookie Domain for the credentials. See 'Crawler User Account' section.
- outputBaseFolder: The base folder that the output should be written to. In Liferay PaaS this can be the Liferay service's persistent storage directory i.e. "/mnt/persistent-storage/". A timestamp based folder will be created within this base folder e.g. /mnt/persistent-storage/siteExport_1726484407262

**Crawler User Account**

- The module is designed to use a non-SSO enabled account to perform the crawling.
- The Language of the user (from Account Settings > General > Information) determines the Display Language applied to the Pages.
- The Instance Settings > User Authentication > 'Allow users to automatically log?' setting must be enabled while the tool is being setup and used. The setting can be disabled afterwards if not required. 
- The User used must be a non-SSO user and must have access to the Site and must have access to all the Pages that are to be exported.
- If necessary, create a Public page and add the 'Sign In' widget. This isn't necessary for the crawler to work but may be required to successfully login as the non-SSO user in a SSO enabled environment during setup. This page can be deleted once the encrypred credentials have been extracted. 
- To get the encrypted emailAddress and password values, perform a non-SSO login as the user in Chrome Incognito Mode with 'Remember Me' checked, then go to Dev Tools > Application > Storage > Cookies:
- The ID cookie value from above should be passed as the emailAddressEnc argument.
- The PASSWORD cookie value from above should be passed as the passwordEnc argument.
- The Domain value from the ID / PASSWORD cookies from above should be passed as the cookieDomain argument.

**Output**

- A .html file for each crawled page will be created in the file system based on the outputBaseFolder and the autogenerated folder.
- The full output path is logged to the Gogo shell console and the Liferay service logs e.g.
```
Output location: /mnt/persistent-storage/siteExport_1726490121400
```
- The HTML file name will be based on the Page Friendly URL with / characters replaced with _. For example Page Friendly URL /mw/test/page-1 will be stored as _mw_test_page-1.html
- On Liferay PaaS the output is written to the Liferay service and are accessible with the Liferay service shell. Depending on the outputBaseFolder the folder may not be persistent. /mnt/persistent-storage/ is persistent.

**Downloading Output in Liferay PaaS**

- If /mnt/persistent-storage/ was used as the outputBaseFolder then the folder and contents can be downloaded with the LCP CLI tool.
- See here for more information on the LCP CLI tool and the download command: https://learn.liferay.com/w/liferay-cloud/reference/command-line-tool#downloading-files-from-the-liferay-service
- Ensure the latest version of the LCP CLI tool is being used.
- Download the crawler output with the following command:
```
lcp files download --prefix /siteExport_1726490121400 --dest c:/temp
```

**Notes**

- The module has been tested in a local environment with JDK 8, Liferay DXP 7.4 U92 and OpenID SSO enabled.
- The module has also been tested in a Liferay PaaS environment with JDK 11, a more recent Liferay DXP quarterly release but without OpenID SSO enabled.
- Javascript, CSS, images and other dependencies are not currently downloaded.
